Below are the resources that we have used to create this project:

1. [Setting up a text summarisation project](https://towardsdatascience.com/setting-up-a-text-summarisation-project-daae41a1aaa3)
    * Code: [GitHub repository](https://github.com/marshmellow77/text-summarisation-project)
2. CNN-Dailymail Dataset: 
    * [TensorFlow Datasets](https://www.tensorflow.org/datasets/catalog/cnn_dailymail)
    * [HuggingFace Datasets](https://huggingface.co/datasets/cnn_dailymail)
    * [GitHub repository to original dataset](https://github.com/abisee/cnn-dailymail)
3. [Ensembling HuggingFace Transformer Models](https://towardsdatascience.com/ensembling-huggingfacetransformer-models-f21c260dbb09)
    * [Google Colab Notebook](https://colab.research.google.com/drive/1SyRrBAudJHiKjHnxXaZT5w_ukA0BmK9X?usp=sharing)
4. [Summarization](https://huggingface.co/course/chapter7/5?fw=pt)
5. [GRUBERT](https://github.com/ZuowenWang0000/GRUBERT-A-GRU-Based-Method-to-Fuse-BERT-Hidden-Layers-for-Twitter-sentiment-analysis)
6. [BERT2BERT](https://colab.research.google.com/drive/1WIk2bxglElfZewOHboPFNj8H44_VAyKE?usp=sharing#scrollTo=8lWokpytzKNk)
7. [Seq2seq Trainer](https://huggingface.co/course/chapter7/5?fw=pt#preprocessing-the-data)
8. [Leveraging Pre-trained Checkpoints for Encoder-Decoder Models](https://colab.research.google.com/drive/1WIk2bxglElfZewOHboPFNj8H44_VAyKE?usp=sharing#scrollTo=JD2jv3GkyjR-)
9. [Fine-tuning a pre-trained model](https://huggingface.co/docs/transformers/training)