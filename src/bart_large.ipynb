{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EyOijJK8e6YN",
        "outputId": "e61dd6e6-b2e1-4101-90fb-651f8438a26e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eAdt_PGtYrel",
        "outputId": "c47f2474-109d-4c32-83d7-4005d9be5cf7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.31.0)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (2.14.4)\n",
            "Requirement already satisfied: rouge-score in /usr/local/lib/python3.10/dist-packages (0.1.2)\n",
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.10/dist-packages (0.21.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.2)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.14.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.16.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.6.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.13.3)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.3.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.1)\n",
            "Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (9.0.0)\n",
            "Requirement already satisfied: dill<0.3.8,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.7)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (1.5.3)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.3.0)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.15)\n",
            "Requirement already satisfied: fsspec[http]>=2021.11.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (2023.6.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.8.5)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.10/dist-packages (from rouge-score) (1.4.0)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (from rouge-score) (3.8.1)\n",
            "Requirement already satisfied: six>=1.14.0 in /usr/local/lib/python3.10/dist-packages (from rouge-score) (1.16.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate) (5.9.5)\n",
            "Requirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (2.0.1+cu118)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (23.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (3.2.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.0.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (4.7.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.7.22)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.10.0->accelerate) (3.27.2)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.10.0->accelerate) (16.0.6)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk->rouge-score) (8.1.6)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk->rouge-score) (1.3.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2023.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.3)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\n"
          ]
        }
      ],
      "source": [
        "# !pip install transformers datasets rouge-score accelerate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m_SJGemHYa5d",
        "outputId": "394eefd1-ae14-49e8-d52d-cc7da8e529cb"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-3-c56484a8e515>:10: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library ðŸ¤— Evaluate: https://huggingface.co/docs/evaluate\n",
            "  metric = datasets.load_metric(\"rouge\")\n"
          ]
        }
      ],
      "source": [
        "from transformers import (\n",
        "  BartForConditionalGeneration, BartTokenizer, DataCollatorForSeq2Seq\n",
        ")\n",
        "import datasets\n",
        "metric = datasets.load_metric(\"rouge\")\n",
        "import nltk\n",
        "import numpy as np\n",
        "import pprint"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m7KD9yEsYa5e"
      },
      "outputs": [],
      "source": [
        "MAX_INPUT_LENGTH = 768\n",
        "MAX_OUTPUT_LENGTH = 100\n",
        "pp = pprint.PrettyPrinter(width=80, compact=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "efpjVJs_Ya5f"
      },
      "outputs": [],
      "source": [
        "# Load the ROUGE scores metric\n",
        "\n",
        "def calc_rouge_scores(candidates, references):\n",
        "    result = metric.compute(predictions = candidates, references = references,\n",
        "        use_stemmer = True)\n",
        "    result = {key: round(value.mid.fmeasure * 100, 1)\n",
        "              for key, value in result.items()}\n",
        "    return result\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P6gAVgdVYa5f"
      },
      "outputs": [],
      "source": [
        "dataset = datasets.load_dataset(\"cnn_dailymail\", \"3.0.0\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "myu3FXaJYa5f",
        "outputId": "a0a2e771-9234-4700-d60e-c169f9e3b528"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "LONDON, England (Reuters) -- Harry Potter star Daniel Radcliffe gains access to a reported Â£20 million ($41.1 million) fortune as he turns 18 on Monday, but he insists the money won't cast a spell on him. Daniel Radcliffe as Harry Potter in \"Harry Potter and the Order of the Phoenix\" To the disappointment of gossip columnists around the world, the young actor says he has no plans to fritter his cash away on fast cars, drink and celebrity parties. \"I don't plan to be one of those people who, as soon as they turn 18, suddenly buy themselves a massive sports car collection or something similar,\" he told an Australian interviewer earlier this month. \"I don't think I'll be particularly extravagant. \"The things I like buying are things that cost about 10 pounds -- books and CDs and DVDs.\" At 18, Radcliffe will be able to gamble in a casino, buy a drink in a pub or see the horror film \"Hostel: Part II,\" currently six places below his number one movie on the UK box office chart. Details of how he'll mark his landmark birthday are under wraps. His agent and publicist had no comment on his plans. \"I'll definitely have some sort of party,\" he said in an interview. \"Hopefully none of you will be reading about it.\" Radcliffe's earnings from the first five Potter films have been held in a trust fund which he has not been able to touch. Despite his growing fame and riches, the actor says he is keeping his feet firmly on the ground. \"People are always looking to say 'kid star goes off the rails,'\" he told reporters last month. \"But I try very hard not to go that way because it would be too easy for them.\" His latest outing as the boy wizard in \"Harry Potter and the Order of the Phoenix\" is breaking records on both sides of the Atlantic and he will reprise the role in the last two films.  Watch I-Reporter give her review of Potter's latest Â» . There is life beyond Potter, however. The Londoner has filmed a TV movie called \"My Boy Jack,\" about author Rudyard Kipling and his son, due for release later this year. He will also appear in \"December Boys,\" an Australian film about four boys who escape an orphanage. Earlier this year, he made his stage debut playing a tortured teenager in Peter Shaffer's \"Equus.\" Meanwhile, he is braced for even closer media scrutiny now that he's legally an adult: \"I just think I'm going to be more sort of fair game,\" he told Reuters. E-mail to a friend . Copyright 2007 Reuters. All rights reserved.This material may not be published, broadcast, rewritten, or redistributed.\n",
            "Harry Potter star Daniel Radcliffe gets Â£20M fortune as he turns 18 Monday .\n",
            "Young actor says he has no plans to fritter his cash away .\n",
            "Radcliffe's earnings from first five Potter films have been held in trust fund .\n"
          ]
        }
      ],
      "source": [
        "print(dataset[\"train\"][\"article\"][0])\n",
        "print(dataset[\"train\"][\"highlights\"][0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jrvKoJSOYa5g"
      },
      "outputs": [],
      "source": [
        "tokenizer = BartTokenizer.from_pretrained(\"facebook/bart-large\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5yUFgxnsxu_P"
      },
      "outputs": [],
      "source": [
        "model = BartForConditionalGeneration.from_pretrained(\"facebook/bart-large\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j35wZ2wqYa5g"
      },
      "outputs": [],
      "source": [
        "def preprocess(examples):\n",
        "    article_split = examples[\"article\"]\n",
        "    labels_split = examples[\"highlights\"]\n",
        "\n",
        "    model_inputs = tokenizer(article_split, max_length = MAX_INPUT_LENGTH, truncation = True, padding = True)\n",
        "    labels = tokenizer(labels_split, max_length = MAX_OUTPUT_LENGTH, truncation = True, padding = True)\n",
        "\n",
        "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
        "    return {\n",
        "        \"attention_mask\": model_inputs[\"attention_mask\"],\n",
        "        \"input_ids\": model_inputs[\"input_ids\"],\n",
        "        \"labels\": model_inputs[\"labels\"]\n",
        "    }\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O7EDMet1app_",
        "outputId": "2f9b6091-6bf2-4963-cb54-a91c02002856"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'input_ids': [0, 574, 4524, 6, 1156, 36, 1251, 43, 480, 3268, 10997, 999, 3028, 7312, 20152, 3077, 899, 7, 10, 431, 984, 844, 153, 1358, 4006, 4, 134, 153, 43, 13016, 25, 37, 4072, 504, 15, 302, 6, 53, 37, 9838, 5, 418, 351, 75, 2471, 10, 8921, 15, 123, 4, 3028, 7312, 20152, 25, 3268, 10997, 11, 22, 29345, 10997, 8, 5, 9729, 9, 5, 5524, 113, 598, 5, 10208, 9, 20445, 6730, 1952, 198, 5, 232, 6, 5, 664, 2701, 161, 37, 34, 117, 708, 7, 856, 3961, 1334, 39, 1055, 409, 15, 1769, 1677, 6, 4076, 8, 6794, 1799, 4, 22, 100, 218, 75, 563, 7, 28, 65, 9, 167, 82, 54, 6, 25, 1010, 25, 51, 1004, 504, 6, 6017, 907, 1235, 10, 2232, 1612, 512, 2783, 50, 402, 1122, 60, 37, 174, 41, 2059, 33242, 656, 42, 353, 4, 22, 100, 218, 75, 206, 38, 581, 28, 1605, 31879, 4, 22, 133, 383, 38, 101, 2159, 32, 383, 14, 701, 59, 158, 2697, 480, 2799, 8, 32570, 8, 37206, 72, 497, 504, 6, 7312, 20152, 40, 28, 441, 7, 23104, 11, 10, 10297, 6, 907, 10, 4076, 11, 10, 8881, 50, 192, 5, 8444, 822, 22, 40534, 523, 35, 4657, 3082, 60, 855, 411, 2127, 874, 39, 346, 65, 1569, 15, 5, 987, 2233, 558, 5966, 4, 10574, 9, 141, 37, 581, 2458, 39, 10043, 4115, 32, 223, 18166, 4, 832, 2936, 8, 285, 661, 56, 117, 1129, 15, 39, 708, 4, 22, 100, 581, 2299, 33, 103, 2345, 9, 537, 60, 37, 26, 11, 41, 1194, 4, 22, 19204, 4146, 9, 47, 40, 28, 2600, 59, 24, 72, 7312, 20152, 18, 1107, 31, 5, 78, 292, 10997, 3541, 33, 57, 547, 11, 10, 2416, 1391, 61, 37, 34, 45, 57, 441, 7, 2842, 4, 2285, 39, 1197, 9444, 8, 33884, 6, 5, 2701, 161, 37, 16, 2396, 39, 1730, 10523, 15, 5, 1255, 4, 22, 4763, 32, 460, 546, 7, 224, 128, 30704, 999, 1411, 160, 5, 26717, 10076, 37, 174, 1865, 94, 353, 4, 22, 1708, 38, 860, 182, 543, 45, 7, 213, 14, 169, 142, 24, 74, 28, 350, 1365, 13, 106, 72, 832, 665, 9913, 25, 5, 2143, 32660, 11, 22, 29345, 10997, 8, 5, 9729, 9, 5, 5524, 113, 16, 3433, 2189, 15, 258, 2380, 9, 5, 5038, 8, 37, 40, 769, 22627, 5, 774, 11, 5, 94, 80, 3541, 4, 1437, 3075, 38, 12, 22026, 10679, 492, 69, 1551, 9, 10997, 18, 665, 9313, 479, 345, 16, 301, 1684, 10997, 6, 959, 4, 20, 928, 254, 34, 10571, 10, 1012, 1569, 373, 22, 2387, 5637, 2722, 60, 59, 2730, 19257, 2675, 11488, 20418, 8, 39, 979, 6, 528, 13, 800, 423, 42, 76, 4, 91, 40, 67, 2082, 11, 22, 17704, 8732, 60, 41, 2059, 822, 59, 237, 2786, 54, 5111, 41, 21297, 1580, 4, 3322, 42, 76, 6, 37, 156, 39, 1289, 2453, 816, 10, 20464, 7044, 11, 2155, 840, 16717, 18, 22, 28568, 687, 72, 2276, 6, 37, 16, 5378, 8988, 13, 190, 2789, 433, 7731, 122, 14, 37, 18, 7818, 41, 4194, 35, 22, 100, 95, 206, 38, 437, 164, 7, 28, 55, 2345, 9, 2105, 177, 60, 37, 174, 1201, 4, 381, 12, 6380, 7, 10, 1441, 479, 1894, 3010, 1201, 4, 404, 659, 1875, 4, 713, 1468, 189, 45, 28, 1027, 6, 2308, 6, 4599, 6, 50, 3802, 4, 2], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Ensuring the tokenizer works\n",
        "example_train_input = dataset[\"train\"][\"article\"][0]\n",
        "tokenizer(example_train_input)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9seUjqhSkYVR"
      },
      "outputs": [],
      "source": [
        "reduced_train_dataset = dataset[\"train\"].select(range(10000))\n",
        "reduced_val_dataset = dataset[\"validation\"].select(range(1000))\n",
        "\n",
        "tokenized_reduced_train_dataset = reduced_train_dataset.map(lambda x: preprocess(x), batched=True, remove_columns=[\"article\", \"highlights\", \"id\"])\n",
        "tokenized_reduced_val_dataset = reduced_val_dataset.map(lambda x: preprocess(x), batched=True, remove_columns=[\"article\", \"highlights\", \"id\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5owFnRYvpIwE"
      },
      "outputs": [],
      "source": [
        "from torch import cuda\n",
        "device = \"cuda\" if cuda.is_available() else \"cpu\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZBxJKkN2aYjm"
      },
      "outputs": [],
      "source": [
        "data_collator = DataCollatorForSeq2Seq(tokenizer, model = model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Kfx7t8oDbeL9"
      },
      "outputs": [],
      "source": [
        "def compute_metrics(eval_pred):\n",
        "    predictions, labels = eval_pred\n",
        "    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
        "    # Replace -100 in the labels as we can't decode them.\n",
        "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
        "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
        "\n",
        "    # Rouge expects a newline after each sentence\n",
        "    decoded_preds = [\"\\n\".join(nltk.sent_tokenize(pred.strip())) for pred in decoded_preds]\n",
        "    decoded_labels = [\"\\n\".join(nltk.sent_tokenize(label.strip())) for label in decoded_labels]\n",
        "\n",
        "    result = metric.compute(predictions=decoded_preds, references=decoded_labels, use_stemmer=True)\n",
        "    # Extract a few results\n",
        "    result = {key: value.mid.fmeasure * 100 for key, value in result.items()}\n",
        "\n",
        "    # Add mean generated length\n",
        "    prediction_lens = [np.count_nonzero(pred != tokenizer.pad_token_id) for pred in predictions]\n",
        "    result[\"gen_len\"] = np.mean(prediction_lens)\n",
        "\n",
        "    return {k: round(v, 4) for k, v in result.items()}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b0jR7rrU4bgX"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "tokenized_reduced_train_dataset.set_format(\"torch\")\n",
        "tokenized_reduced_val_dataset.set_format(\"torch\")\n",
        "BATCH_SIZE = 12\n",
        "train_dataloader = DataLoader(\n",
        "    tokenized_reduced_train_dataset,\n",
        "    shuffle = True,\n",
        "    collate_fn = data_collator,\n",
        "    batch_size = BATCH_SIZE\n",
        ")\n",
        "\n",
        "eval_dataloader = DataLoader(\n",
        "    tokenized_reduced_val_dataset,\n",
        "    collate_fn = data_collator,\n",
        "    batch_size = BATCH_SIZE\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XR20gU2n4wC8",
        "outputId": "c4b4b8e6-2bce-4557-a9ee-cd36c2d7bd21"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "from transformers import AdamW\n",
        "\n",
        "optimizer = AdamW(model.parameters(), lr = 1e-4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jhMgxEJd5OnN"
      },
      "outputs": [],
      "source": [
        "import locale\n",
        "locale.getpreferredencoding = lambda: \"UTF-8\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oZb7_UqI49wA"
      },
      "outputs": [],
      "source": [
        "from accelerate import Accelerator\n",
        "\n",
        "accelerator = Accelerator()\n",
        "model, optimizer, train_dataloader, eval_dataloader = accelerator.prepare(\n",
        "    model, optimizer, train_dataloader, eval_dataloader\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9k5I-6ik5Bgh"
      },
      "outputs": [],
      "source": [
        "from transformers import get_scheduler\n",
        "\n",
        "num_train_epochs = 5\n",
        "num_update_steps_per_epoch = len(train_dataloader)\n",
        "num_training_steps = num_train_epochs * num_update_steps_per_epoch\n",
        "\n",
        "lr_scheduler = get_scheduler(\n",
        "    \"linear\",\n",
        "    optimizer=optimizer,\n",
        "    num_warmup_steps=0,\n",
        "    num_training_steps=num_training_steps,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zFkHZK2B08AW"
      },
      "outputs": [],
      "source": [
        "def postprocess_text(preds, labels):\n",
        "    preds = [pred.strip() for pred in preds]\n",
        "    labels = [label.strip() for label in labels]\n",
        "\n",
        "    # ROUGE expects a newline after each sentence\n",
        "    preds = [\"\\n\".join(nltk.sent_tokenize(pred)) for pred in preds]\n",
        "    labels = [\"\\n\".join(nltk.sent_tokenize(label)) for label in labels]\n",
        "\n",
        "    return preds, labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MuhVmVSU5ajw"
      },
      "outputs": [],
      "source": [
        "output_dir = \"/content/drive/MyDrive/FinTech Society News Summaries Project\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MSkLLO2TBmuc",
        "outputId": "49add882-2792-4d8d-ea6f-eb99a4785055"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 24,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "nltk.download(\"punkt\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oRICKvFAqvMj",
        "outputId": "a225d213-d3b2-4cfe-f5e7-eef81e5f9b4f"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "BartForConditionalGeneration(\n",
              "  (model): BartModel(\n",
              "    (shared): Embedding(50265, 1024, padding_idx=1)\n",
              "    (encoder): BartEncoder(\n",
              "      (embed_tokens): Embedding(50265, 1024, padding_idx=1)\n",
              "      (embed_positions): BartLearnedPositionalEmbedding(1026, 1024)\n",
              "      (layers): ModuleList(\n",
              "        (0-11): 12 x BartEncoderLayer(\n",
              "          (self_attn): BartAttention(\n",
              "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "          )\n",
              "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "          (activation_fn): GELUActivation()\n",
              "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "      )\n",
              "      (layernorm_embedding): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "    )\n",
              "    (decoder): BartDecoder(\n",
              "      (embed_tokens): Embedding(50265, 1024, padding_idx=1)\n",
              "      (embed_positions): BartLearnedPositionalEmbedding(1026, 1024)\n",
              "      (layers): ModuleList(\n",
              "        (0-11): 12 x BartDecoderLayer(\n",
              "          (self_attn): BartAttention(\n",
              "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "          )\n",
              "          (activation_fn): GELUActivation()\n",
              "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "          (encoder_attn): BartAttention(\n",
              "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "          )\n",
              "          (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "      )\n",
              "      (layernorm_embedding): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "    )\n",
              "  )\n",
              "  (lm_head): Linear(in_features=1024, out_features=50265, bias=False)\n",
              ")"
            ]
          },
          "execution_count": 25,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/",
          "height": 158,
          "referenced_widgets": [
            "2248cc702bb74d1d95e6438610f300b7",
            "471f7eb02f0849bab26f5cf289e62aa4",
            "d839ac67fe86472b9669f2fae03c8cb8",
            "79791b547fbb48d7bf5a0f3a09804336",
            "864eda07945f49c3abdbf078c1d83f3a",
            "1eb1055e9ac0484fb4c684dff7f48f22",
            "0b6a0e0495834badb4cf3f6e572a9d61",
            "41744ba32bb24805a5e348ad6e8ad063",
            "e975d9f5f43242ddbef86111ffbc0fb6",
            "294eee6e6c734b029a7f52b159ee6d3a",
            "9b3066e7329e4aa38b60d0bcb2d4fc76"
          ]
        },
        "id": "_-A_K9Gl5JEB",
        "outputId": "d0950ca4-00dc-4967-bbea-5cb9dc648021"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "2248cc702bb74d1d95e6438610f300b7",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/4170 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1369: UserWarning: Using `max_length`'s default (20) to control the generation length. This behaviour is deprecated and will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 0: {'rouge1': 23.0847, 'rouge2': 9.5761, 'rougeL': 19.3544, 'rougeLsum': 21.2192}\n",
            "Epoch 1: {'rouge1': 22.296, 'rouge2': 9.5221, 'rougeL': 18.7969, 'rougeLsum': 20.642}\n",
            "Epoch 2: {'rouge1': 23.1146, 'rouge2': 9.939, 'rougeL': 19.442, 'rougeLsum': 21.3431}\n",
            "Epoch 3: {'rouge1': 23.4268, 'rouge2': 9.7363, 'rougeL': 19.4775, 'rougeLsum': 21.4599}\n",
            "Epoch 4: {'rouge1': 23.4293, 'rouge2': 9.8499, 'rougeL': 19.4313, 'rougeLsum': 21.5058}\n"
          ]
        }
      ],
      "source": [
        "from tqdm.auto import tqdm\n",
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "progress_bar = tqdm(range(num_training_steps))\n",
        "results = []\n",
        "for epoch in range(num_train_epochs):\n",
        "\n",
        "    model.train()\n",
        "    for step, batch in enumerate(train_dataloader):\n",
        "        batch = batch.to(device)\n",
        "        outputs = model(**batch)\n",
        "        loss = outputs.loss\n",
        "        accelerator.backward(loss)\n",
        "        del loss\n",
        "        del outputs\n",
        "        optimizer.step()\n",
        "        lr_scheduler.step()\n",
        "        optimizer.zero_grad()\n",
        "        progress_bar.update(1)\n",
        "\n",
        "    # Evaluation\n",
        "    model.eval()\n",
        "    for step, batch in enumerate(eval_dataloader):\n",
        "        batch = batch.to(device)\n",
        "        with torch.no_grad():\n",
        "            generated_tokens = accelerator.unwrap_model(model).generate(\n",
        "                batch[\"input_ids\"],\n",
        "                attention_mask=batch[\"attention_mask\"],\n",
        "            )\n",
        "\n",
        "            generated_tokens = accelerator.pad_across_processes(\n",
        "                generated_tokens, dim=1, pad_index=tokenizer.pad_token_id\n",
        "            )\n",
        "            labels = batch[\"labels\"]\n",
        "\n",
        "            # If we did not pad to max length, we need to pad the labels too\n",
        "            labels = accelerator.pad_across_processes(\n",
        "                batch[\"labels\"], dim=1, pad_index=tokenizer.pad_token_id\n",
        "            )\n",
        "\n",
        "            generated_tokens = accelerator.gather(generated_tokens).cpu().numpy()\n",
        "            labels = accelerator.gather(labels).cpu().numpy()\n",
        "\n",
        "            # Replace -100 in the labels as we can't decode them\n",
        "            labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
        "            if isinstance(generated_tokens, tuple):\n",
        "                generated_tokens = generated_tokens[0]\n",
        "            decoded_preds = tokenizer.batch_decode(\n",
        "                generated_tokens, skip_special_tokens=True\n",
        "            )\n",
        "            decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
        "\n",
        "            decoded_preds, decoded_labels = postprocess_text(\n",
        "                decoded_preds, decoded_labels\n",
        "            )\n",
        "\n",
        "            metric.add_batch(predictions=decoded_preds, references=decoded_labels)\n",
        "\n",
        "    # Compute metrics\n",
        "    result = metric.compute()\n",
        "    # Extract the median ROUGE scores\n",
        "    result = {key: value.mid.fmeasure * 100 for key, value in result.items()}\n",
        "    result = {k: round(v, 4) for k, v in result.items()}\n",
        "    print(f\"Epoch {epoch}:\", result)\n",
        "    results.append(result)\n",
        "\n",
        "    # Save and upload\n",
        "    accelerator.wait_for_everyone()\n",
        "    unwrapped_model = accelerator.unwrap_model(model)\n",
        "    unwrapped_model.save_pretrained(output_dir, save_function=accelerator.save)\n",
        "    tokenizer.save_pretrained(output_dir)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "tBpRwA99Yv7O"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "torch.save(model, os.path.join(\"/content/drive/MyDrive/FinTech Society News Summaries Project\", \"finetuned-bart-large.pt\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "0_u_raTd91ic",
        "outputId": "35ca0529-7bee-45c8-d3d9-786c02cc903d"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'AMMAN, Jordan (CNN) -- In the sunbathed schoolyard of the Shmisani Institute for Girls in Amman, Jordan, principal Sanaa Abu Harb makes an announcement over the speaker system. Iraqi students at the Shmisani school in Amman gather around a teacher. One in 5 students there is Iraqi. \"All Iraqi girls come outside now. All Iraqi girls. Iraqi girls only!\" she repeats several times, making sure the message is clear and waving away Jordanian pupils attracted by the commotion. Dozens of girls in green apron-like uniforms pour out into the courtyard and cluster on the top level of a stone staircase overlooking a concrete playground. Harb wants the CNN crew to see how many Iraqi refugee girls her school is accommodating. This school year, she says, 145 students are Iraqi -- roughly 20 percent of the students at this state-funded institution -- with another 40 Iraqi children on a waiting list.  Watch Iraqi girls describe a long way from home Â» . The reason behind the jump in the number of Iraqis at the school is a new government policy: For the first time since the start of the Iraq war, Jordan is allowing all Iraqi children -- regardless of refugee status -- to enroll in state-funded schools. Simply, this means that even illegal refugees with no paperwork can send their kids to school with no questions asked. The move is cementing a massive population shift in the Middle East. More than 2.2 million Iraqis have fled the violence in their homeland, most of them seeking refuge in neighboring Jordan and Syria, according to humanitarian officials. Jordanian Minister of Education Khalid Touqan says he expects Jordan to accommodate 40,000 to 50,000 Iraqi students this year. That\\'s more than double the number of Iraqi children enrolled in public school two years ago. Harb, on the front line of the phenomenon, says the influx is putting a strain on her school. Even with some U.N. and U.S. aid to Jordan, there\\'s still not enough money. \"We need more teachers here, more resources, more buildings, more chairs for all Iraqi students and our students,\" she says. In a nearby neighborhood, in the study room of the Ahmed Toukan School for Boys, a handful of Iraqi kids talk of their experience living far from home. Seated at a rectangular table covered with a red and white tablecloth, the boys tell stories of horror and displacement. Eighteen-year-old Qutaiba lost five immediate family members before moving to Jordan to try to live a normal life. Matter-of-factly and with a straight-ahead stare, he repeats the number: \"Five members.\" Most of the boys and young men from Iraq have missed several years of school -- up to a four-year educational gap that will delay not only their high school graduation, but also their entry into the workforce. All say, though, that they feel lucky to have gotten out, even if the violence in their country means always having to be on the move, ready to live far from home and away from loved ones. \"It\\'s not strange for me to be in the middle of people I don\\'t know,\" says eleventh grader Ziad Tarek Al Shamsi. \"I had friends in Iraq when I was small, I left them. In America, I left them. I came here, I left them.\" He pauses: \"But you have to miss your country.\" The United Nations High Commissioner for Refugees estimates up to 250,000 school-age Iraqi children are in Jordan. Many of them are enrolled in private institutions. But as families run out of money they had when they left Iraq, they turn to public schools. Even so, more than a month into the new academic year, fewer Iraqi families than first anticipated enrolled their kids in schools this year. According to the charity Save the Children, 21,000 Iraqi children have so far enrolled in Jordanian classrooms. As a result, the government extended the deadline for student applications and cut down on the required paperwork for Iraqi families. The lower registration numbers were attributed in part to illegal refugees\\' fears of being identified through their children\\'s school records. Regardless of what the final number will be this year, the population shift in the Middle East is, according to UNHCR head Antonio Guterres, the largest urban refugee situation in the world. Iraqi families are changing the social fabric of Jordanian society. About 10 percent of Jordan\\'s population is now made up of Iraqi refugees -- the estimates range from 500,000 to 750,000 of them. The schoolchildren are living examples of how the Iraq war may permanently change the Middle East. \"Iraqi children will be incorporated and integrated within our mainstream line of education,\" says Touqan, the education minister. \"We will not run a parallel system of education.\" E-mail to a friend .'"
            ]
          },
          "execution_count": 28,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "dataset[\"train\"][\"article\"][50]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "Mbtx_1sHhqwJ",
        "outputId": "f67df232-96e3-4f19-dd71-130026bd556e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Jordan opens school doors to all Iraqi children regardless of refugee status.\n",
            "Principal: \"All Iraqi girls come outside now. Iraqi girls only\"\n",
            "Education minister expects to accommodate 40,000 to 50,000 Iraqi students this school year.\n",
            "More than 2.2 million Iraqis have fled the violence in their homeland, most seeking refuge in neighboring Jordan and Syria.\n",
            "Jordan estimates up to 250,000 school-age Iraqi children are in Jordan.\n"
          ]
        }
      ],
      "source": [
        "input_ids = tokenizer(dataset[\"train\"][\"article\"][50], return_tensors = \"pt\").input_ids\n",
        "generated_ids = model.generate(input_ids.to(device), do_sample = True, max_length = 100)\n",
        "summary = tokenizer.decode(generated_ids.squeeze(), skip_special_tokens = True)\n",
        "print(summary)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "VwDRUQahngR1",
        "outputId": "71098c85-7979-476f-c7b8-44cf1abb7f93"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'LONDON, England (Reuters) -- Harry Potter star Daniel Radcliffe gains access to a reported Â£20 million ($41.1 million) fortune as he turns 18 on Monday, but he insists the money won\\'t cast a spell on him. Daniel Radcliffe as Harry Potter in \"Harry Potter and the Order of the Phoenix\" To the disappointment of gossip columnists around the world, the young actor says he has no plans to fritter his cash away on fast cars, drink and celebrity parties. \"I don\\'t plan to be one of those people who, as soon as they turn 18, suddenly buy themselves a massive sports car collection or something similar,\" he told an Australian interviewer earlier this month. \"I don\\'t think I\\'ll be particularly extravagant. \"The things I like buying are things that cost about 10 pounds -- books and CDs and DVDs.\" At 18, Radcliffe will be able to gamble in a casino, buy a drink in a pub or see the horror film \"Hostel: Part II,\" currently six places below his number one movie on the UK box office chart. Details of how he\\'ll mark his landmark birthday are under wraps. His agent and publicist had no comment on his plans. \"I\\'ll definitely have some sort of party,\" he said in an interview. \"Hopefully none of you will be reading about it.\" Radcliffe\\'s earnings from the first five Potter films have been held in a trust fund which he has not been able to touch. Despite his growing fame and riches, the actor says he is keeping his feet firmly on the ground. \"People are always looking to say \\'kid star goes off the rails,\\'\" he told reporters last month. \"But I try very hard not to go that way because it would be too easy for them.\" His latest outing as the boy wizard in \"Harry Potter and the Order of the Phoenix\" is breaking records on both sides of the Atlantic and he will reprise the role in the last two films.  Watch I-Reporter give her review of Potter\\'s latest Â» . There is life beyond Potter, however. The Londoner has filmed a TV movie called \"My Boy Jack,\" about author Rudyard Kipling and his son, due for release later this year. He will also appear in \"December Boys,\" an Australian film about four boys who escape an orphanage. Earlier this year, he made his stage debut playing a tortured teenager in Peter Shaffer\\'s \"Equus.\" Meanwhile, he is braced for even closer media scrutiny now that he\\'s legally an adult: \"I just think I\\'m going to be more sort of fair game,\" he told Reuters. E-mail to a friend . Copyright 2007 Reuters. All rights reserved.This material may not be published, broadcast, rewritten, or redistributed.'"
            ]
          },
          "execution_count": 30,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "dataset[\"train\"][\"article\"][0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "NeZbjaUin3ar"
      },
      "outputs": [],
      "source": [
        "import pickle\n",
        "from pathlib import Path\n",
        "FOLDER_PATH = Path(\"/content/drive/MyDrive/FinTech Society News Summaries Project\")\n",
        "RESULTS_FILE = Path(\"bart_large_results.pkl\")\n",
        "RESULTS_PATH = FOLDER_PATH / RESULTS_FILE\n",
        "with RESULTS_PATH.open(\"wb\") as f:\n",
        "  pickle.dump(results, f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qkH4tA8MrTdf"
      },
      "outputs": [],
      "source": [
        "# from google.colab import runtime\n",
        "# runtime.unassign()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Tc6j94xtrT3r"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuClass": "premium",
      "machine_shape": "hm",
      "provenance": []
    },
    "gpuClass": "premium",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.9"
    },
    "vscode": {
      "interpreter": {
        "hash": "19c9bf76c51d3f844768fbf5e1a5e96f9504eaf0ac3d372613599d2b5ca4150e"
      }
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "0b6a0e0495834badb4cf3f6e572a9d61": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1eb1055e9ac0484fb4c684dff7f48f22": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2248cc702bb74d1d95e6438610f300b7": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_471f7eb02f0849bab26f5cf289e62aa4",
              "IPY_MODEL_d839ac67fe86472b9669f2fae03c8cb8",
              "IPY_MODEL_79791b547fbb48d7bf5a0f3a09804336"
            ],
            "layout": "IPY_MODEL_864eda07945f49c3abdbf078c1d83f3a"
          }
        },
        "294eee6e6c734b029a7f52b159ee6d3a": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "41744ba32bb24805a5e348ad6e8ad063": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "471f7eb02f0849bab26f5cf289e62aa4": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1eb1055e9ac0484fb4c684dff7f48f22",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_0b6a0e0495834badb4cf3f6e572a9d61",
            "value": " 60%"
          }
        },
        "79791b547fbb48d7bf5a0f3a09804336": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_294eee6e6c734b029a7f52b159ee6d3a",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_9b3066e7329e4aa38b60d0bcb2d4fc76",
            "value": " 2510/4170 [42:45&lt;1:16:24,  2.76s/it]"
          }
        },
        "864eda07945f49c3abdbf078c1d83f3a": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9b3066e7329e4aa38b60d0bcb2d4fc76": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d839ac67fe86472b9669f2fae03c8cb8": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_41744ba32bb24805a5e348ad6e8ad063",
            "max": 4170,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_e975d9f5f43242ddbef86111ffbc0fb6",
            "value": 2510
          }
        },
        "e975d9f5f43242ddbef86111ffbc0fb6": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
